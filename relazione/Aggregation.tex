\section{Data aggregation}
\label{aggregation}
In our model, we dealt with interactions between different nodes in the considered area, rescaling them and considering them as probabilities.
However, often the little aggregation time lead to very noisy datasets, among which it was difficult to operate achieving meaningful results.

So, to get more realistic results and to deal with the very high level of noise in the dataset,
we decided to gather and average the strength of interaction between couples of nodes in several 10-minutes periods
considering a semantically meaningful aggregation in time (not alway contiguous, e.g. we aggregated all monday
mornings periods).

To achieve this, we developed a procedure composed by two consecutive logical map reduce computations executable within the Hadoop environment.
The input of the \emph{Time Aggregation} phase is the raw dataset, while the output is a graph of probabilities of interaction between zones of the considered area
over a certain period of interest.
Here follows the pseudo-code of the used map, combine and reduce operations, to be seen as subsequent logical steps.
\begin{verbatim}
FilterMap(key, value):
// input takes in input all dataset files
// implementeted in aggregated_graphs.FilterMapper.java
  d = parse(value)
  for(interesting_period in all_periods)
  if (d.timestamp belongs_to interesting_period)
  	k = (interesting_period,d.sourceNode,d.destNode)
  	v = d.value
    emit(k, v)
    return;

AverageReduce(key,values):
// implemented in aggregated_graphs.AverageReducer.java
  sum = count = 0
  for v in values:
  	sum += v
  	count++
  (id,num,source,dest) = parse(key)
  avg = sum / num
  emit((id,source),(dest,value))

IdentityMap(key, value):
  emit(key, value)

ProbabilityReduce(key, values):
  sum = 0
  a_list = []
  for v in values:
    (dest, weight) = parse(v)
    sum += weight
  	a_list.append((dest,weight))
  for a in a_list:
    emit((key.src, a.dest), a.weight / sum)
\end{verbatim}

\subsection{Implementation details}

\paragraph{Disk usage}: During the initial executions, our testing environment
could not successfully compute aggregations over huge periods, because of lack of free disk space.
The problem was due to the fact that, in the first map, even though the filter would reduce in general the size of the input data,
this reduction was not enough to fit disks on certain nodes of the cluster before going on with the conseguent reduce phase.
To cope with this issue, we went through a number of different approaches, to finally discover that a \emph{"classic"} splitting technique, along with the benefits provided by the combiner, would result in the best solution for our environment.
\begin{enumerate}
\item \textbf{Combiners} a combiner has been defined to shrink the first map output size. The combiner performs a sum over all strengths in the same aggregation period and, of course, with same source and destination. 
The output of the combiner is a graph for each aggregation period in which the strength is the sum of all strength.
This solution, for an aggregation with size of n slots, could reduce the output of each map task up to $1/n$ of the original size without the combiner. 
The pseudo-code of the combiner used is depicted below:
\begin{verbatim}
FilterCombine(key, value):
	sum = 0;
	for v in value
		sum += v
	context.write(key, sum)
\end{verbatim}
\item \textbf{GZip} another approach that we tried, and thereafter discarded, was to use GZip as compression codec for data going out of the map. From experimental results, we noticed that this approach, altough not providing enough benefits
to cope with disk utilization issues, slowed down our computation significantly.
\item \textbf{Change in split size} thanks to the combiner, the biggest is the number of time slots gathered in a single aggregation, the biggest is the shrinking in data. 
Therefore, we tried to increase the size of the splits to be given to a single map task to exploit this reduction. We progressively increased split size from 128MB to 1GB and then to contain a single whole file, which was defined as not splittable.
However, we noticed that this method, while decreasing the efficiency in the cluster utilization (as only fewer processing units could be used together) and in the completion time of this phase, was still not enough to cope with our disk issues.
\item \textbf{Divide and conquer} the latest, and best approach to solve the disk space issue, was to change the logic in the driver. What was previously the first map-reduce phase, has been decomposed into several little map-reduce phases, each one taking a parametric
chunk of the initial data. This led to benefits from the point of view of disk usage, of completion time and of cluster utilization.
\end{enumerate}

\subsection{Java code}
The Java code for this part of the work, can be found at the following url.... . %todo: insert url

