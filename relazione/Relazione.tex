\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{float}
\restylefloat{figure}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

\author{Michele Carignani, Alessandro Lenzi}
\title{Community discovery in Milan}
\subtitle{project for the course of Distributed Enabling Platforms}

\makeindex

\begin{document}

\maketitle
\tableofcontents
\newpage

\begin{abstract}
The aim of the project is to analyze and then visualize telecommunication data
in order to discover real-world communities basing on the number of mobile telecommunications (calls and text messages) 
between different geographical areas. We design and implemented 
a parallel computation as a pipeline of
map reduce jobs onto the Hadoop distributed computation
framewor and environment\footnote{http://hadoop.apache.org/}.
In sections \ref{thedataset} and \ref{ds_analysis} of this report, we describe the original dataset
and analyze its properties. Then in section \ref{aggregation}
we explain how we rearranged the data in
order to reduce the noise and improve the tractability and robustness of the
community discovery phase. Sections \ref{approaches} and \ref{mcl} will then
briefly describe two different approaches for community discovery
(finding strongly connected components and markovian clustering)
and in detail describe strategy and implementation of the latter which has
proved to give more significant results.
Finally section \ref{results} will collect results and conclusion of the developed project.
All the implementation code for the project can be found in this
\href{''https://github.com/michele-carignani/hadoop-markov-clustering/''}{online repository}.
\end{abstract}
\newpage
%% Dataset and analysis
\input{Dataset.tex}
\newpage
\input{Idea.tex}

%% How aggregated graphs are generated
\input{Aggregation.tex}


\section{Communities discovery approaches}
\label{approaches}

Once the aggregated graph is produced (as list of weighted edges) is time to compute
the communities (i.e. the clusters) over it.

Two different approaches were developed and tested. 
As we will see, the second will give the
expected results, while the first will fail.

\subsection{Tarjan Connected Components algorithm}
%todo: explain cuts
Defining the communities as connected components onto the graph\footnote{A connected component
in a graph $A = (N,E)$ is a subset of nodes $A' \in A$  s.t. each node in A' is connected to 
all the other nodes in A' 
}, the first idea was to apply Tarjan's algorithm for connected components, whose pseudo code is in \ref{alg:tarjan}.


In our initial approach, a mere visit, ordered on the arc identifier (i.e. \texttt{for i = 0 to 10000 do strongconnect(i)}) was performed. This visit, though executed with different cuts over the probability of the arcs, evidenced
a strong bias of the order of visit on the found strongly connected components.
As an example, with a cut on probability 0.005, a huge SCC is found, containing almost all nodes. Only few nodes remain
outside this CFC, and some small aggregations can be found between them.
So our second step has been achieving a visiting strategy consistent with the effective traffic measured during the day.
To do so, we used the measurements of the total activity of grids in order to establish a visiting order to be 
followed during the procedure.
\begin{figure}
\begin{verbatim}
input: graph G = (V, E)
output: set of strongly connected components (sets of vertices)
index := 0
S := empty
for each v in V do
    if (v.index is undefined) then
        strongconnect(v)
		
function strongconnect(v):
    v.index := index
    v.lowlink := index
    index := index + 1
    S.push(v)
    for each (v, w) in E:
        if (w.index is undefined):
            strongconnect(w)
        v.lowlink := min(v.lowlink, w.lowlink)
		else if (w is in S):
            v.lowlink := min(v.lowlink, w.index)
    
    if (v.lowlink = v.index):
        start a new strongly connected component
	do
        w := S.pop()
        add w to current strongly connected component
    until (w = v)
    output the current strongly connected component
\end{verbatim}
\caption{Tarjan Strongly Connected Components algorithm}
\label{alg:tarjan}
\end{figure}
We performed the following attempts and different visits on our graph:
\begin{enumerate}
\item Nodes visited for increasing outgoing hourly traffic, with arcs selected with increasing probability. 
\item Nodes visited for decreasing outgoing hourly traffic, with arcs ordered with increasing probability.
\item Nodes visited for decreasing outgoing hourly traffic, with arcs ordered with decreasing probability.
\item Nodes visited for increasing outgoing hourly traffic, with arcs ordered with decreasing probability.
\end{enumerate}
\begin{figure}
\centering
\includegraphics[scale=0.5]{tarjanscc.png}
\caption{Strongly connected components found with Tarjan Algorithm.From left to right, top to bottom the hours considered are 12, 13, 14, 15.
The visit has been performed with increasing hourly traffic and arcs selected with increasing probability. 
The cut has been performed at value 0.05.}
\label{fig:tarjan1}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.6]{tarjanscc2.png}
\caption{Stronglu connected components found with Tarjan Algorithms. From left to right, top to bottom the hours considered are 0, 1, 2 and 3.
The visits have been performed with increasing hourly traffic and arcs selected with increasing probability.
Cut performed at value 0.05}
\label{fig:tarjan2}
\end{figure}

The image \ref{fig:tarjan1} shows the found strongly connected components in the most trafficated hours 
of the analysis. As it is possible to notice by looking to \ref{fig:analysis}, in fact, during
these hours arcs values are very concentrated near the mean, and thus most of them will be cutted out, making
most of the strongly connected components to disappear in the most trafficated hours of the day.
The results are slightly better in less trafficated hours as shown in \ref{fig:tarjan2}, in which the variance is higher and thus a wider number
of arcs will "save himself" from the performed cutting.
In other attempts, we tried to modify the threshold so that it would save more arcs for the computation,
but we have not been able to find an appropriate threshold leading to a large enough number of clusters
and with an acceptable size.\\
In fact, in most cases, a too low threshold led to few huge connected components whilst too high led to few and very small
connected components.\\
The approach described above, in our opinion, could not lead to the desired results because of two reasons:
\begin{itemize}
\item First, the visiting order, though more meaningful, was still establishing a bias in the search of the components because
of the "paths" eliminated by the original Tarjan algorithm
\item and second, the "static" threshold did not adapt well to changing traffic along the days.
\end{itemize}
To overcome this issues, we decided to implement a small variation of the Tarjan SCC algorithm, in which visited nodes not
becoming part of a strongly connected component could be visited again while searching for others, thus increasing the
complexity of the algorithm but with the advantage of reducing the visit ordering bias.
\\
The other modification that we implemented was that of percentile-based cuts. In this approach, for every hour, the probability
distribution has been calculated and only arc probabilities in a certain percentile have been held, while the others
have been cutted as it was done before with a "static" threshold. This allows for a more fine-grained cutting, allowing
to keep more arcs also in more trafficated hours of the day.

\begin{figure}
\includegraphics[scale=0.8]{tarjan3.png}
\caption{Strongly connected components found cutting at the 99-th percentile. Hours depicted are, from left to right and from top to bottom, 12-17.}
\label{fig:tarjan3}
\end{figure}

In \ref{fig:tarjan3} this modified algorithm has been tried with a cut on the 99-th precentile. In the plot
for the SCC found in this case, it is possible to see that most of them are geographically localized (as we expected),
but still their size is very small. \\
However, only few components were able to "survive" across several hours, and are mostly localized in the outskirts 
of the city. \\
Other attempts have been made to find suitable strongly connected components, but also small modification in percentiles
led either to very noise results (with almost all zones in the same component) or to empty results. 
From this approach, we understood that
\begin{enumerate}
\item Very high cuts are needed, because of the high connectivity degree of the graph
\item The statistics of arcs probabilities, however, seem to indicate the existance of zones calling themselves significantly more frequently than others.
\item Visiting strategy has a very strong bias, and probably never considering twice the same arc could lead to eliminate some interesting strongly connected components
\item The number of components is almost always lower than the expected one, however they are uniformly spreaded along the space taken into consideration
\item In different hours, the components vary in their positions, possibly indicating different users behaviours in different hours
\item Specially during hours in which we expected less traffic, components tend to move towards the outskirts, possibly denoting clusters belonging to small towns near Milan
\item The found components are not stable during consecutive hours but they appear and disappear. In the beginning we thought this was due to the "static" cut, but the reproposition
of this behaviour with the percentile denotes that the reason must be either a very noisy dataset or high variation in the users behaviours during the day.
\end{enumerate}

As previously said in Chapter 2, all of this research led us to the conclusion that the best approach was to find a sort of "average behaviour" to analyse and
to move to a different approach, in which communities are seen as clusters. 
To do so, we chose the \emph{Markov Clustering} algorithm, which is known in litterature for the purpose of finding communities in graphs.


For a more complete dissertation on what we did using this approach for discovering communities, we invite you to refer to the attacched file \texttt{CfcSuGrafiOrari.pdf}, in Italian.

\subsection{Markov Clustering}

We then looked at a diffferent approach: Markov Clustering based on the Markov Clustering Algorithm by Stijn van Dongen\footnote{\url{http://micans.org/mcl/}}.

The idea was to reduce noise and emphasize relations in a more structured way
by multiplying the adjancy matrix of the graph by itself until the number of
non-null elements in each row is very low (reduce the number of edges) and with
values probability weight close to 1 (taking the most probable connections).

Therefore, the pseudocode of the general algorithm is:
\begin{figure}
\begin{verbatim}
   G is a graph
   r = 4
   set M_1 to be the matrix of random walks on G

   while (change) {
      M_2 =  M_1 * M_1
      M_1 =  inflation(M_2)
      change   = difference(M_1, M_2)
   }

   set CLUSTERING as the components of M_1
\end{verbatim}
\caption{Markov Clustering pseudocode}
\end{figure}

The algorithm is based on the assumption that $G$ is a graph
were edges are weighted with probabilities and the sum
of all edges exiting a node is 1. Thus the adjacency matrix
of the graph is stochastic by rows.

The matrix can be therefore seen as the matrix of probabilities
of transitioning from a state to another state in
a Markov Process, also called random walk matrix.

The Markovian Clustering Algorithm simulates fluxes within the
graphs and calculates, with n iterations, the probabilities of going in
n steps from each node to another one. Taking this simulation to the limit
(or better, to a appropriate approximation of the limit) identifies
the most probable destinations of the flux from each node.

\input{MarkovClustering.tex}

\section{Results}
\label{results}

We present here the results of the community discovery analysis
over two aggregation periods: Mondays and Wednesdays mornings.

This means that we launched the Aggregation module and 2 constructed 
the average probability graphs, one with the connections strength
calcolated as the average of all the 10-minutes intervals included in
the hours 7-13 of all Mondays in November and December 2013. The other
one with the same hours but belonging to all the Wednesdays contained
in the observation period.

We chose this 2 aggregation in order to find similarities among them and to
identify ''work-related'' communities.

The results are graphically shown in figures \ref{fig:patch_mon}, \ref{fig:patch_wed},
\ref{fig:arcs_mon}, \ref{fig:arcs_wed}, firstly as a ''heat map'' over the grid
(each color representing a differnt component) then by drawing all arcs (in
order to identify the directions of calls).

The heat maps clearly display that:
\begin{itemize}
\item the number of communities detected over our dataset population is about
100 and this number seems acceptable (as an order of magnitude);
\item Components are - in general - concentrated in the same geographical area;
\item Components get smaller near the city centre and larger going toward the
country-side and this seems reasonable given the different density of population
between them;
\item There is a dramatic difference w.r.t. the results of the discovery
computed with the Tarjan algorithm in term of number and dimension of the
respectively found components.
\item Most of all: the communities discovered in different and independent
analysis (such as the ones on Mondays and Wednesdays) look really similar
in terms of distribution and size. This corroborates the goodness of the
algorithm design and implementation as we could possibly be able to
discover work-related communities.
\end{itemize}

In conclusion, the opinion of the authors is that community discovery
onto a geographical area based on mobile telecommunication data
is possible and that the Markovian random walk approach gives much better
results than visit based approachies (even with different strategies applied)
such as Tarjan's algorithm. This is probably due to the assence,
in Markovian Clustering, of any bias w.r.t. how to and from where start
and proceed in the visit. Therefore, also other biased machine learning
approaches (such as K-Means) would not probably compete with Markov
Clustering in terms of quality of the output, in this scenario.

This work leaves the door open for more analysis, in particular to study
different communities in different aggregation types, and the authors
suggest this road to be covered in the future.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{monday.png}
\caption{}
\label{fig:patch_mon}




\includegraphics[scale=0.2]{clusters.png}
\caption{}
\label{fig:arcs_mon}




\includegraphics[scale=0.2]{wednesday.png}
\caption{}
\label{fig:patch_wed}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.2]{wednesday_arcs.png}
\caption{}
\label{fig:arcs_wed}
\end{figure}

\


\end{document}
\begin{figure}